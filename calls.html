---
id: calls
layout: default
style: true
---
<h2>Call for papers</h2>
<p>
    Learning models defining recursive computations, like automata and formal grammars, is the core of the field called Grammatical Inference (GI).
    The expressive power of these models and the complexity of the associated computational problems are major research topics within the mathematical logic and computer science communities.
    Historically, there has been little interaction between the GI and LICS communities, though recently some important results started to bridge the gap between both worlds, including applications of learning to formal verification and model checking, and (co-)algebraic formulations of automata and grammar learning algorithms.
</p>

<p>
    The goal of this workshop is to bring together experts on logic who could benefit from grammatical inference tools, and researchers in grammatical inference who could find in logic and verification new fruitful applications for their methods.
</p>

<p>
    We invite submissions of recent work, including preliminary research, related to the theme of the workshop.
    Similarly to how main machine learning conferences and workshops are organized, all accepted abstracts will be part of a poster session held during the workshop.
    Additionally, the Program Committee will select a subset of the abstracts for oral presentation.
    At least one author of each accepted abstract is expected to represent it at the workshop.
</p>

<div id="specifics">
    <div id="topics">
        <h3>Topics of interest</h3>
        <p>
            <ul>
                <li>Computational complexity of learning problems involving automata and formal languages.</li>
                <li>Algorithms and frameworks for learning models representing language classes inside and outside the Chomsky hierarchy, including tree and graph grammars.</li>
                <li>Learning problems involving models with additional structure, including numeric weights, inputs/outputs such as transducers, register automata, timed automata, Markov reward and decision processes, and semi-hidden Markov models.</li>
                <li>Theoretical studies of learnable classes of languages/representations.</li>
                <li>Logical and relational aspects of learning and grammatical inference.</li>
                <li>Relations between automata and recurrent neural networks.</li>
                <li>Active learning of finite state machines and formal languages.</li>
                <li>Methods for estimating probability distributions over strings, trees, graphs, or any data used as input for symbolic models.</li>
                <li>Applications of learning to formal verification and (statistical) model checking.</li>
                <li>Theoretical studies of learnable classes of languages/representations.</li>
                <li>Metrics and other error measures between automata or formal languages.</li>
            </ul>
        </p>
    </div>

    <div>
        <div id="instructions">
            <h3>Submission instructions</h3>
            <p>
                Submissions in the form of extended abstracts must be at most 8 single-column pages long (plus at most two for bibliography and possible appendixes) and must be submitted in the JMLR/PMLR format.
                The LaTeX style file is <a href="https://ctan.org/tex-archive/macros/latex/contrib/jmlr">available on CTAN</a>.
            </p>

            <p>
                We do accept submissions of work recently published or currently under review.
            </p>

            <p>
                Submissions are handled <a href="https://easychair.org/conferences/?conf=learnaut2019">through EasyChair</a>.
            </p>
        </div>

        <div id="dates">
            <h3>Important dates</h3>
            <ul>
                {% for date in site.data.dates %}
                <li>{{ date.name }}: {{ date.value }}</li>
                {% endfor %}
            </ul>
        </div>
    </div>
</div>
